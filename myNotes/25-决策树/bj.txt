机器学习
一、概述
1.什么是机器学习？
人工智能：通过人工的方法，实现或者近似实现某些需要人类智能处理的问题，都可以称为人工智能。
机器学习：一个计算机程序在完成任务T之后，获得经验E，而该经验的效果可以通过P得以表现，如果随着T的增加，借助P来表现的E也可以同步增进，则称这样的程序为机器学习系统。
自我完善、自我修正、自我增强。
2.为什么需要机器学习？
1)简化或者替代人工方式的模式识别，易于系统的开发维护和升级换代。
2)对于那些算法过于复杂，或者没有明确解法的问题，机器学习系统具有得天独厚的优势。
3)借鉴机器学习的过程，反向推理出隐藏在业务数据背后的规则――数据挖掘。
3.机器学习的类型
1)有监督学习、无监督学习、半监督学习和强化学习
2)批量学习和增量学习
3)基于实例的学习和基于模型的学习
4.机器学习的流程
数据采集
数据清洗             数据
-----------------------
数据预处理  
选择模型
训练模型
验证模型      机器学习
-----------------------
使用模型             业务
维护和升级
二、数据预处理
import sklearn.preprocessing as sp
样本矩阵
                    输入数据             输出数据
               _____特征_____
             /       |        |      \
          身高  体重  年龄  性别
样本1 1.7    60    25     男    -> 8000
样本2 1.5    50    20     女    -> 6000
... 
1.均值移除(标准化)
特征A：10+-5
特征B：10000+-5000
特征淹没
通过算法调整令样本矩阵中每一列(特征)的平均值为0，标准差为1。这样一来，所有特征对最终模型的预测结果都有接近一致的贡献，模型对每个特征的倾向性更加均衡。
[a b c]
m=(a+b+c)/3, s=sqrt(((a-m)^2+(b-m)^2+(c-m)^2)/3)
[a' b' c']
a'=a-m
b'=b-m
c'=c-m
m'
=(a'+b'+c')/3
=(a-m+b-m+c-m)/3
=(a+b+c-3m)/3
=(a+b+c)/3-m
=m-m
=0
[a" b" c"]
a"=a'/s
b"=b'/s
c"=c'/s
m"=0
s"
=sqrt((a"^2+b"^2+c"^2)/3)
=sqrt((a'^2+b'^2+c'^2)/(3s^2))
=sqrt(((a-m)^2+(b-m)^2+(c-m)^2)/(3s^2))
=sqrt(3s^2/(3s^2))
=1
sp.scale(原始样本矩阵)->经过均值移除后的样本矩阵
代码：std.py
2.范围缩放
90/150  80/100  5/5
将样本矩阵每一列的元素经过某种线性变换，使得所有列的元素都处在同样的范围区间内。
k x + b = y
k col_min + b = min   \  -> k b
k col_max + b = max  /
/ col_min 1 \ x / k \ = / min \
\ col_max 1/    \ b /    \ max /
---------------   -----    --------
           a               x             b
                            = np.linalg.solve(a, b)
                            = np.linalg.lstsq(a, b)[0]
范围缩放器 = sp.MinMaxScaler(
    feature_range=(min, max))
范围缩放器.fit_transform(原始样本矩阵)
    ->经过范围缩放后的样本矩阵
有时候也把以[0, 1]区间作为目标范围的范围缩放称为"归一化"
代码：mms.py
3.归一化
           Python C/C++ Java PHP
2016  20          30        40    10    /100
2017  30          20        30    10    /90
2018  10            5          1      0    /16
用每个样本各个特征值除以该样本所有特征值绝对值之和，以占比的形式来表现特征。
sp.normalize(原始样本矩阵, norm='l1')
    ->经过归一化后的样本矩阵
l1 - l1范数，矢量诸元素的绝对值之和
l2 - l2范数，矢量诸元素的(绝对值的)平方之和
...
ln - ln范数，矢量诸元素的绝对值的n次方之和
代码：nor.py
4.二值化
根据事先给定阈值，将样本矩阵中高于阈值的元素设置为1，否则设置为0，得到一个完全由1和0组成的二值矩阵。
二值化器 = sp.Binarizer(threshold=阈值)
二值化器.transform(原始样本矩阵)
    ->经过二值化后的样本矩阵
代码：bin.py
5.独热编码
用一个只包含一个1和若干个0的序列来表达每个特征值的编码方式，借此既保留了样本矩阵的所有细节，同时又得到一个只含有1和0的稀疏矩阵，既可以提高模型的容错性，同时还能节省内存空间。
1        3        2
7        5        4
1        8        6
7        3        9
----------------------
1:10  3:100 2:1000
7:01  5:010 4:0100
          8:001 6:0010
                     9:0001
----------------------
101001000
010100100
100010010
011000001
独热编码器 = sp.OneHotEncoder(
    sparse=是否紧缩(缺省True), dtype=类型)
独热编码器.fit_transform(原始样本矩阵)
    ->经过独热编码后的样本矩阵
代码：ohe.py
6.标签编码
文本形式的特征值->数值形式的特征值
其编码数值源于标签字符串的字典排序，与标签本身的含义无关
职位 车
员工 toyota - 0
组长 ford     - 1
经理 audi     - 2
老板 bmw    - 3
标签编码器 = sp.LabelEncoder()
标签编码器.fit_transform(原始样本矩阵)
    ->经过标签编码后的样本矩阵
标签编码器.inverse_transform(经过标签编码后的样本矩阵)
    ->原始样本矩阵
代码：lab.py
三、机器学习的基本问题
1.回归问题：由已知的分布于连续域中的输入和输出，通过不断地模型训练，找到输入和输出之间的联系，通常这种联系可以通过一个函数方程被形式化，如：y=w0+w1x+w2x^2...，当提供未知输出的输入时，就可以根据以上函数方程，预测出与之对应的连续域输出。
2.分类问题：如果将回归问题中的输出从连续域变为离散域，那么该问题就是一个分类问题。
3.聚类问题：从已知的输入中寻找某种模式，比如相似性，根据该模式将输入划分为不同的集群，并对新的输入应用同样的划分方式，以确定其归属的集群。
4.降维问题：从大量的特征中选择那些对模型预测最关键的少量特征，以降低输入样本的维度，提高模型的性能。
四、一元线性回归
1.预测函数
输入  输出
0       1
1       3
2       5
3       7
4       9
...
y=1+2 x
10 -> 21
y=w0+w1x
任务就是寻找预测函数中的模型参数w0和w1，以满足输入和输出之间的联系。
2.单样本误差
x -> [y=w0+w1x] -> y'    y -> e=1/2(y-y')^2
3.总样本误差
E = SIGMA[1/2(y-y')^2]
4.损失函数
Loss(w0,w1)=SIGMA[1/2(y-(w0+w1x))^2]
任务就是寻找可以使损失函数取得最小值的模型参数w0和w1。
5.梯度下降法寻优
随机选择一组模型参数w0和w1
计算损失函数在该模型参数处的梯度<-+
[DLoss/Dwo, DLoss/Dw1]                |
计算与该梯度反方向的修正步长            |
[-nDLoss/Dwo, -nDLoss/Dw1]        |
计算下一组模型参数                             |
w0=w0-nDLoss/Dw0                       |
w1=w1-nDLoss/Dw1---------------+
直到满足迭代终止条件：
迭代足够多次；
损失值已经足够小；
损失值已经不再明显减少。
Loss = SIGMA[1/2(y-y')^2], y'=w0+w1x
DLoss/Dw0
=SIGMA[D(1/2(y-y')^2)/Dw0]
=SIGMA[(y-y')D(y-y')/Dw0]
=SIGMA[(y-y')(Dy/Dw0-Dy'/Dw0)]
=-SIGMA[(y-y')(Dy'/Dw0)]
=-SIGMA[(y-y')]
DLoss/Dw1
=SIGMA[D(1/2(y-y')^2)/Dw1]
...
=-SIGMA[(y-y')(Dy'/Dw1)]
=-SIGMA[(y-y')x]
代码：gd.py
import sklearn.linear_model as lm
线性回归器 = lm.LinearRegression()
线性回归器.fit(已知输入, 已知输出) # 计算模型参数
线性回归器.predict(新的输入)->新的输出
代码：line.py
模型的转储与载入：pickle
代码：dump.py、load.py
五、岭回归
Loss(w0, w1)=SIGMA[1/2(y-(w0+w1x))^2]
                         +正则强度 * f(w0, w1)
通过正则的方法，即在损失函数中加入正则项，以减弱模型参数对熟练数据的匹配度，借以规避少数明显偏移正常范围的异常样本影响模型的回归效果。
代码：rdg.py
六、多项式回归
多元线性：    y=w0+w1x1+w2x2+w3x3+...+wnxn
                       ^ x1 = x^1
                        | x2 = x^2
                        | ...
                        | xn = x^n
一元多项式：y=w0+w1x+w2x^2+w3x^3+...+wnx^n
x->多项式特征扩展器 -x1...xn-> 线性回归器->w0...wn
      \______________________________________/
                                   管线
代码：poly.py
七、决策树
1.基本原理
相似的输入导致相似的输出。
年龄：青年-1，中年-2，老年-3
学历：专科-1，本科-2，硕士-3，博士-4
经验：缺乏-1，一般-2，丰富-3，资深-4
性别：男-1，女-2
薪资：1-低，2-中，3-高，4-超高
年龄  学历  工作经验  性别 -> 薪资
1       1      1               2          5000     1
1       2      2               1          8000     2
2       3      3               2          10000   3
3       4      4               1          30000   4
...
------------------------------------------
1       2      2               1          ?
回归――平均 \ 结合特征的相
分类――投票 / 似程度做加权
随着子表的划分，信息熵越来越小，信息量越来越大，
数据越来越有序。
11123                2...                  3...
12211                2...                  3...
11221                2...                  3...

11... 12... 13... 14... 
11... 12... 13... 14...
11... 12... 13... 14...
依次选择原始样本矩阵中的每一列，构建相应特征值相同的若干子表树，在叶级子表中所有特征值都是相同的，对于未知输出的输入，按照同样的规则，归属到某个叶级子表，将该子表中各样本的输出按照取平均(回归)或者取投票(分类)的方法，计算预测输出。
2.工程优化
1)根据信息熵的减少量计算每个特征对预测结果的影响程度，信息熵减少量越大的特征对预测结果的影响也越大。
2)根据上一步计算出的影响程度，按照从大到小的顺序，选择划分子表的特征依据，即优先选择影响程度最大的特征。
3)根据事先给定的条件，提前结束子表的划分过程，借以简化决策树的结构，缩短构建和搜索的时间，在预测精度牺牲不大的前提下，提高模型性能。
3.集合算法
1)所谓集合算法，亦称集合弱学习方法，其核心思想就是，通过平均或者投票，将多个不同学习方法的结论加以综合，给出一个相对可靠预测结果。所选择的弱学习方法，在算法或数据上应该具备足够分散性，以体现相对不同的倾向性，这样得出的综合结论才能够更加泛化。
2)基于决策树的集合算法，就是按照某种规则，构建多棵彼此不同的决策树模型，分别给出针对未知样本的预测结果，最后通过平均或投票得到相对综合的结论。
3)根据构建多棵决策树所依据的规则不同，基于决策树的集合算法可被分为以下几种：
A.自助聚合：从原始训练样本中，以有放回或无放回抽样的方式，随机选取部分样本，构建一棵决策树，重复以上过程，得到若干棵决策输，以此弱化某些强势样本对预测结果的影响力，提高模型精度。
B.随机森林：如果在自助聚合的基础上，每次构建决策树时，不但随机选择样本(行)，而且其特征(列)也是随机选择的，则称为随机森林。
C.正向激励：首先为训练样本分配相等的权重，构建第一棵决策树，用该决策树对训练样本进行预测，为预测错误的样本提升权重，再次构建下一棵决策树，以此类推，得到针对每个样本拥有不同权重的多棵决策树。
代码：house.py
5.特征重要性
决策树模型，在确定划分子表优先选择特征的过程中，需要根据最大熵减原则，确定划分子表的依据，因此，作为学习模型的副产品，可以得到每个特征对于输出的影响力度，即特征重要性：feature_importances_，该输出与模型算法有关。
代码：fi.py
学习模型关于特征重要性的计算，除了与选择的算法有关以外，还与数据的采集粒度有关。
代码：bike.py
八、人工分类
       输入          输出
--------------   -----
特征1  特征2
3         1           0
2         5           1
1         8           1
6         4           0
5         2           0
3         5           1
4         7           1
4        -1          0
------------------
6         8           1
5         1           0
代码：simple.py
九、逻辑分类
y = w0+w1x1+w2x2
连续的预测值->离散的预测值
   [-oo, +oo]->{0, 1}      1
逻辑函数：sigmoid = ---------
                                    1+e^-y
非线性化
                            1
y = -----------------------------
       1+e^-(w0+w1x1+w2x2)
3         1 -> 0.2 0
2         5 -> 0.8 1
1         8 -> 0.7 1
6         4 -> 0.3 0
...
将预测函数的输出看做输入被划分为1类的概率，择概率大的类别作为预测结果。
代码：log2.py
多元分类
   /\
走  不走
           /\
     骑车 不骑车
                   /\
             坐车 不坐车
                           /\
                     开车 不开车
                             ...
                A       B       C
... -> A 1 0.7 0 0.2 0 0.1 -> A
... -> B 0 0.1 1 0.8 0 0.4 -> B
... -> C 0 0.3 0 0.3 1 0.9 -> C
代码：log3.py
十、朴素贝叶斯分类
1.算法原理
1000
10
1
P(遇到美女)=10/1000=0.01
P(被美女爱)=1/10=0.1

贝叶斯定理：
              P(A)P(B|A)
P(A|B)=-------------
                    P(B)
P(A|B)P(B)=P(B|A)P(A)
        P(A,B)=P(B,A)
3         1 -> 0
                    1
              P(C)P(x|C)
P(C|x)=-------------
                    P(x)
P(C)P(x|C)
=P(C,x)
=P(C,x1,x2)
=P(x1,x2,C)
=P(x1|x2,C)P(x2,C)
=P(x1|x2,C)P(x2|C)P(C)
朴素：条件独立假设
=P(x1|C)P(x2|C)P(C)
代码：nb.py
2.划分训练集和测试集
import sklearn.model_selection as ms
ms.train_test_split(
    输入集, 输出集, test_size=测试集占比,
    ramdom_state=随机种子)
    ->训练输入, 测试输入, 训练输出, 测试输出
代码：split.py
3.交叉验证
ms.cross_val_score(模型, 输入集, 输出集, cv=折叠数,
                                  scoring=指标名)->指标值数组
指标：
1)精确度(accuracy)：分类正确的样本数/总样本数
2)查准率(precision_weighted)：针对每一个类别，预测正确的样本数比上预测出来的样本数
3)召回率(recall_weighted)：针对每一个类别，预测正确的样本数比上实际存在的样本数
4)f1得分(f1_weighted)：
   2x查准率x召回率/(查准率+召回率)
在交叉验证过程中，针对每一个折叠，计算所有类别的查准率、召回率或者f1得分，然后取各类别相应指标值的平均数，作为这一个折叠的评估指标，然后再将所有折叠的评估指标以数组的形式返回调用者。
代码：cv.py
4.混淆矩阵
每一行和每一列分别对应样本输出中的每一个类别，行表示实际类别，列表示预测类别。
    A B C
A  5 0 0
B  0 6 0 - 理想混淆矩阵
C  0 0 7
    A B C
A 3  1 1
B 0  4 2
C 0  0 7
主对角线上的值
---------------- = 查准率  \
该值所在列的和                  \  -> F1得分
主对角线上的值                   /
----------------- = 召回率 /
该值所在行的和
sm.confusion_matrix(实际输出, 预测输出)->混淆矩阵
代码：cm.py
5.分类报告
sm.classification_report(实际输出, 预测输出)->分类报告
代码：cr.py
十一、决策树分类
以叶子表的投票结果确定预测类别。
代码：car.py






