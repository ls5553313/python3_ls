机器学习
一、概述
1.什么是机器学习？
人工智能：通过人工的方法，实现或者近似实现某些需要人类智能处理的问题，都可以称为人工智能。
机器学习：一个计算机程序在完成任务T之后，获得经验E，而该经验的效果可以通过P得以表现，如果随着T的增加，借助P来表现的E也可以同步增进，则称这样的程序为机器学习系统。
自我完善、自我修正、自我增强。
2.为什么需要机器学习？
1)简化或者替代人工方式的模式识别，易于系统的开发维护和升级换代。
2)对于那些算法过于复杂，或者没有明确解法的问题，机器学习系统具有得天独厚的优势。
3)借鉴机器学习的过程，反向推理出隐藏在业务数据背后的规则――数据挖掘。
3.机器学习的类型
1)有监督学习、无监督学习、半监督学习和强化学习
2)批量学习和增量学习
3)基于实例的学习和基于模型的学习
4.机器学习的流程
数据采集
数据清洗             数据
-----------------------
数据预处理  
选择模型
训练模型
验证模型      机器学习
-----------------------
使用模型             业务
维护和升级
二、数据预处理
import sklearn.preprocessing as sp
样本矩阵
                    输入数据             输出数据
               _____特征_____
             /       |        |      \
          身高  体重  年龄  性别
样本1 1.7    60    25     男    -> 8000
样本2 1.5    50    20     女    -> 6000
... 
1.均值移除(标准化)
特征A：10+-5
特征B：10000+-5000
特征淹没
通过算法调整令样本矩阵中每一列(特征)的平均值为0，标准差为1。这样一来，所有特征对最终模型的预测结果都有接近一致的贡献，模型对每个特征的倾向性更加均衡。
[a b c]
m=(a+b+c)/3, s=sqrt(((a-m)^2+(b-m)^2+(c-m)^2)/3)
[a' b' c']
a'=a-m
b'=b-m
c'=c-m
m'
=(a'+b'+c')/3
=(a-m+b-m+c-m)/3
=(a+b+c-3m)/3
=(a+b+c)/3-m
=m-m
=0
[a" b" c"]
a"=a'/s
b"=b'/s
c"=c'/s
m"=0
s"
=sqrt((a"^2+b"^2+c"^2)/3)
=sqrt((a'^2+b'^2+c'^2)/(3s^2))
=sqrt(((a-m)^2+(b-m)^2+(c-m)^2)/(3s^2))
=sqrt(3s^2/(3s^2))
=1
sp.scale(原始样本矩阵)->经过均值移除后的样本矩阵
代码：std.py
2.范围缩放
90/150  80/100  5/5
将样本矩阵每一列的元素经过某种线性变换，使得所有列的元素都处在同样的范围区间内。
k x + b = y
k col_min + b = min   \  -> k b
k col_max + b = max  /
/ col_min 1 \ x / k \ = / min \
\ col_max 1/    \ b /    \ max /
---------------   -----    --------
           a               x             b
                            = np.linalg.solve(a, b)
                            = np.linalg.lstsq(a, b)[0]
范围缩放器 = sp.MinMaxScaler(
    feature_range=(min, max))
范围缩放器.fit_transform(原始样本矩阵)
    ->经过范围缩放后的样本矩阵
有时候也把以[0, 1]区间作为目标范围的范围缩放称为"归一化"
代码：mms.py
3.归一化
           Python C/C++ Java PHP
2016  20          30        40    10    /100
2017  30          20        30    10    /90
2018  10            5          1      0    /16
用每个样本各个特征值除以该样本所有特征值绝对值之和，以占比的形式来表现特征。
sp.normalize(原始样本矩阵, norm='l1')
    ->经过归一化后的样本矩阵
l1 - l1范数，矢量诸元素的绝对值之和
l2 - l2范数，矢量诸元素的(绝对值的)平方之和
...
ln - ln范数，矢量诸元素的绝对值的n次方之和
代码：nor.py
4.二值化
根据事先给定阈值，将样本矩阵中高于阈值的元素设置为1，否则设置为0，得到一个完全由1和0组成的二值矩阵。
二值化器 = sp.Binarizer(threshold=阈值)
二值化器.transform(原始样本矩阵)
    ->经过二值化后的样本矩阵
代码：bin.py
5.独热编码
用一个只包含一个1和若干个0的序列来表达每个特征值的编码方式，借此既保留了样本矩阵的所有细节，同时又得到一个只含有1和0的稀疏矩阵，既可以提高模型的容错性，同时还能节省内存空间。
1        3        2
7        5        4
1        8        6
7        3        9
----------------------
1:10  3:100 2:1000
7:01  5:010 4:0100
          8:001 6:0010
                     9:0001
----------------------
101001000
010100100
100010010
011000001
独热编码器 = sp.OneHotEncoder(
    sparse=是否紧缩(缺省True), dtype=类型)
独热编码器.fit_transform(原始样本矩阵)
    ->经过独热编码后的样本矩阵
代码：ohe.py
6.标签编码
文本形式的特征值->数值形式的特征值
其编码数值源于标签字符串的字典排序，与标签本身的含义无关
职位 车
员工 toyota - 0
组长 ford     - 1
经理 audi     - 2
老板 bmw    - 3
标签编码器 = sp.LabelEncoder()
标签编码器.fit_transform(原始样本矩阵)
    ->经过标签编码后的样本矩阵
标签编码器.inverse_transform(经过标签编码后的样本矩阵)
    ->原始样本矩阵
代码：lab.py
三、机器学习的基本问题
1.回归问题：由已知的分布于连续域中的输入和输出，通过不断地模型训练，找到输入和输出之间的联系，通常这种联系可以通过一个函数方程被形式化，如：y=w0+w1x+w2x^2...，当提供未知输出的输入时，就可以根据以上函数方程，预测出与之对应的连续域输出。
2.分类问题：如果将回归问题中的输出从连续域变为离散域，那么该问题就是一个分类问题。
3.聚类问题：从已知的输入中寻找某种模式，比如相似性，根据该模式将输入划分为不同的集群，并对新的输入应用同样的划分方式，以确定其归属的集群。
4.降维问题：从大量的特征中选择那些对模型预测最关键的少量特征，以降低输入样本的维度，提高模型的性能。
四、一元线性回归
1.预测函数
输入  输出
0       1
1       3
2       5
3       7
4       9
...
y=1+2 x
10 -> 21
y=w0+w1x
任务就是寻找预测函数中的模型参数w0和w1，以满足输入和输出之间的联系。
2.单样本误差
x -> [y=w0+w1x] -> y'    y -> e=1/2(y-y')^2
3.总样本误差
E = SIGMA[1/2(y-y')^2]
4.损失函数
Loss(w0,w1)=SIGMA[1/2(y-(w0+w1x))^2]
任务就是寻找可以使损失函数取得最小值的模型参数w0和w1。
5.梯度下降法寻优
随机选择一组模型参数w0和w1
计算损失函数在该模型参数处的梯度<-+
[DLoss/Dwo, DLoss/Dw1]                |
计算与该梯度反方向的修正步长            |
[-nDLoss/Dwo, -nDLoss/Dw1]        |
计算下一组模型参数                             |
w0=w0-nDLoss/Dw0                       |
w1=w1-nDLoss/Dw1---------------+
直到满足迭代终止条件：
迭代足够多次；
损失值已经足够小；
损失值已经不再明显减少。
Loss = SIGMA[1/2(y-y')^2], y'=w0+w1x
DLoss/Dw0
=SIGMA[D(1/2(y-y')^2)/Dw0]
=SIGMA[(y-y')D(y-y')/Dw0]
=SIGMA[(y-y')(Dy/Dw0-Dy'/Dw0)]
=-SIGMA[(y-y')(Dy'/Dw0)]
=-SIGMA[(y-y')]
DLoss/Dw1
=SIGMA[D(1/2(y-y')^2)/Dw1]
...
=-SIGMA[(y-y')(Dy'/Dw1)]
=-SIGMA[(y-y')x]
代码：gd.py
import sklearn.linear_model as lm
线性回归器 = lm.LinearRegression()
线性回归器.fit(已知输入, 已知输出) # 计算模型参数
线性回归器.predict(新的输入)->新的输出
代码：line.py
模型的转储与载入：pickle
代码：dump.py、load.py
五、岭回归
Loss(w0, w1)=SIGMA[1/2(y-(w0+w1x))^2]
                         +正则强度 * f(w0, w1)
通过正则的方法，即在损失函数中加入正则项，以减弱模型参数对熟练数据的匹配度，借以规避少数明显偏移正常范围的异常样本影响模型的回归效果。
代码：rdg.py
六、多项式回归
多元线性：    y=w0+w1x1+w2x2+w3x3+...+wnxn
                       ^ x1 = x^1
                        | x2 = x^2
                        | ...
                        | xn = x^n
一元多项式：y=w0+w1x+w2x^2+w3x^3+...+wnx^n
x->多项式特征扩展器 -x1...xn-> 线性回归器->w0...wn
      \______________________________________/
                                   管线
代码：poly.py
七、决策树
1.基本原理
相似的输入导致相似的输出。
年龄：青年-1，中年-2，老年-3
学历：专科-1，本科-2，硕士-3，博士-4
经验：缺乏-1，一般-2，丰富-3，资深-4
性别：男-1，女-2
薪资：1-低，2-中，3-高，4-超高
年龄  学历  工作经验  性别 -> 薪资
1       1      1               2          5000     1
1       2      2               1          8000     2
2       3      3               2          10000   3
3       4      4               1          30000   4
...
------------------------------------------
1       2      2               1          ?
回归――平均 \ 结合特征的相
分类――投票 / 似程度做加权
随着子表的划分，信息熵越来越小，信息量越来越大，
数据越来越有序。
11123                2...                  3...
12211                2...                  3...
11221                2...                  3...

11... 12... 13... 14... 
11... 12... 13... 14...
11... 12... 13... 14...
依次选择原始样本矩阵中的每一列，构建相应特征值相同的若干子表树，在叶级子表中所有特征值都是相同的，对于未知输出的输入，按照同样的规则，归属到某个叶级子表，将该子表中各样本的输出按照取平均(回归)或者取投票(分类)的方法，计算预测输出。
2.工程优化
1)根据信息熵的减少量计算每个特征对预测结果的影响程度，信息熵减少量越大的特征对预测结果的影响也越大。
2)根据上一步计算出的影响程度，按照从大到小的顺序，选择划分子表的特征依据，即优先选择影响程度最大的特征。
3)根据事先给定的条件，提前结束子表的划分过程，借以简化决策树的结构，缩短构建和搜索的时间，在预测精度牺牲不大的前提下，提高模型性能。
3.集合算法
1)所谓集合算法，亦称集合弱学习方法，其核心思想就是，通过平均或者投票，将多个不同学习方法的结论加以综合，给出一个相对可靠预测结果。所选择的弱学习方法，在算法或数据上应该具备足够分散性，以体现相对不同的倾向性，这样得出的综合结论才能够更加泛化。
2)基于决策树的集合算法，就是按照某种规则，构建多棵彼此不同的决策树模型，分别给出针对未知样本的预测结果，最后通过平均或投票得到相对综合的结论。
3)根据构建多棵决策树所依据的规则不同，基于决策树的集合算法可被分为以下几种：
A.自助聚合：从原始训练样本中，以有放回或无放回抽样的方式，随机选取部分样本，构建一棵决策树，重复以上过程，得到若干棵决策输，以此弱化某些强势样本对预测结果的影响力，提高模型精度。
B.随机森林：如果在自助聚合的基础上，每次构建决策树时，不但随机选择样本(行)，而且其特征(列)也是随机选择的，则称为随机森林。
C.正向激励：首先为训练样本分配相等的权重，构建第一棵决策树，用该决策树对训练样本进行预测，为预测错误的样本提升权重，再次构建下一棵决策树，以此类推，得到针对每个样本拥有不同权重的多棵决策树。
代码：house.py
5.特征重要性
决策树模型，在确定划分子表优先选择特征的过程中，需要根据最大熵减原则，确定划分子表的依据，因此，作为学习模型的副产品，可以得到每个特征对于输出的影响力度，即特征重要性：feature_importances_，该输出与模型算法有关。
代码：fi.py
学习模型关于特征重要性的计算，除了与选择的算法有关以外，还与数据的采集粒度有关。
代码：bike.py
八、人工分类
       输入          输出
--------------   -----
特征1  特征2
3         1           0
2         5           1
1         8           1
6         4           0
5         2           0
3         5           1
4         7           1
4        -1          0
------------------
6         8           1
5         1           0
代码：simple.py
九、逻辑分类
y = w0+w1x1+w2x2
连续的预测值->离散的预测值
   [-oo, +oo]->{0, 1}      1
逻辑函数：sigmoid = ---------
                                    1+e^-y
非线性化
                            1
y = -----------------------------
       1+e^-(w0+w1x1+w2x2)
3         1 -> 0.2 0
2         5 -> 0.8 1
1         8 -> 0.7 1
6         4 -> 0.3 0
...
将预测函数的输出看做输入被划分为1类的概率，择概率大的类别作为预测结果。
代码：log2.py
多元分类
   /\
走  不走
           /\
     骑车 不骑车
                   /\
             坐车 不坐车
                           /\
                     开车 不开车
                             ...
                A       B       C
... -> A 1 0.7 0 0.2 0 0.1 -> A
... -> B 0 0.1 1 0.8 0 0.4 -> B
... -> C 0 0.3 0 0.3 1 0.9 -> C
代码：log3.py
十、朴素贝叶斯分类
1.算法原理
1000
10
1
P(遇到美女)=10/1000=0.01
P(被美女爱)=1/10=0.1

贝叶斯定理：
              P(A)P(B|A)
P(A|B)=-------------
                    P(B)
P(A|B)P(B)=P(B|A)P(A)
        P(A,B)=P(B,A)
3         1 -> 0
                    1
              P(C)P(x|C)
P(C|x)=-------------
                    P(x)
P(C)P(x|C)
=P(C,x)
=P(C,x1,x2)
=P(x1,x2,C)
=P(x1|x2,C)P(x2,C)
=P(x1|x2,C)P(x2|C)P(C)
朴素：条件独立假设
=P(x1|C)P(x2|C)P(C)
代码：nb.py
2.划分训练集和测试集
import sklearn.model_selection as ms
ms.train_test_split(
    输入集, 输出集, test_size=测试集占比,
    ramdom_state=随机种子)
    ->训练输入, 测试输入, 训练输出, 测试输出
代码：split.py
3.交叉验证
ms.cross_val_score(模型, 输入集, 输出集, cv=折叠数,
                                  scoring=指标名)->指标值数组
指标：
1)精确度(accuracy)：分类正确的样本数/总样本数
2)查准率(precision_weighted)：针对每一个类别，预测正确的样本数比上预测出来的样本数
3)召回率(recall_weighted)：针对每一个类别，预测正确的样本数比上实际存在的样本数
4)f1得分(f1_weighted)：
   2x查准率x召回率/(查准率+召回率)
在交叉验证过程中，针对每一个折叠，计算所有类别的查准率、召回率或者f1得分，然后取各类别相应指标值的平均数，作为这一个折叠的评估指标，然后再将所有折叠的评估指标以数组的形式返回调用者。
代码：cv.py
4.混淆矩阵
每一行和每一列分别对应样本输出中的每一个类别，行表示实际类别，列表示预测类别。
    A B C
A  5 0 0
B  0 6 0 - 理想混淆矩阵
C  0 0 7
    A B C
A 3  1 1
B 0  4 2
C 0  0 7
主对角线上的值
---------------- = 查准率  \
该值所在列的和                  \  -> F1得分
主对角线上的值                   /
----------------- = 召回率 /
该值所在行的和
sm.confusion_matrix(实际输出, 预测输出)->混淆矩阵
代码：cm.py
5.分类报告
sm.classification_report(实际输出, 预测输出)->分类报告
代码：cr.py
十一、决策树分类
1.以叶子表的投票结果确定预测类别。
代码：car.py
2.验证曲线：模型性能 = f(超参数)
ms.validation_curve(模型, 输入集, 输出集, 超参数名,
    超参数序列，cv=折叠数)->训练集得分, 测试集得分
                        CV1 CV2 ...
超参数取值1->0.9   0.7  ... ->mean()
超参数取值2->0.6   0.8  ... ->mean() <- max
..     ^
        |
     best
代码：vc.py
3.学习曲线：模型性能 = f(训练集大小)
ms.learning_curve(模型, 输入集, 输出集,
    train_sizes=训练集大小序列，cv=折叠数)
    ->训练集大小序列, 训练集得分, 测试集得分
训练集大小序列=[0.9 0.8 0.7 0.6 0.5]
         CV1 CV2 ...
0.9->0.9   0.7  ... ->mean()
0.8->0.6   0.8  ... ->mean() <- max
...^
    |
 best
代码：lc.py
4.针对不同形式的特征选择不同类型的编码器
代码：inc.py
十二、支持向量机(SVM)
1.原理
1)寻求最优分类边界：
正确：对大部分样本可以正确地划分类别。
泛化：最大化支持向量间距。
公平：与支持向量等距。
简单：线性，直线或平面，分割超平面。
2)基于核函数的升维变换：
通过名为核函数的特征变换，增加新的特征，使得低维度空间中的线性不可分问题变为高维度空间中的线性可分问题。
2.不同核函数的分类效果
1)线性核函数：linear，不通过核函数进行维度提升，尽在原始维度空间中寻求线性分类边界。
代码：svm_line.py
2)多项式核函数：poly，通过多项式函数增加原始样本特征的高次方幂
x1 x2 -> y
x1 x2 x1^2 x1x2 x2^2 -> y 2次多项式升维
x1 x2 x1^3 x1^2x2 x1x2^2 x2^3 -> y 3次多项式升维
代码：svm_poly.py
3)径向基核函数：rbf，通过高斯分布函数增加原始样本特征的分布概率
代码：svm_rbf.py
3.样本类别均衡化
..., class_weight='balanced', ...
通过类别权重的均衡化，使所占比例较小的样本权重较高，而所占比例较大的样本权重较低，以此平均化不同类别样本对分类模型的贡献，提高模型性能。
代码：svm_bal.py
4.置信概率
根据样本与分类边界的距离远近，对其预测类别的可信程度进行量化，离边界越近的样本，置信概率越高，反之，离边界越远的样本，置信概率越低。
构造model时指定参数，probability=True
model.predict_proba(输入样本矩阵)->置信概率矩阵
预测结果(model.predict()函数返回)：
样本1 类别1
样本2 类别1
样本3 类别2
置信概率矩阵：
          类别1 类别2
样本1 0.8     0.2
样本2 0.9     0.1
样本3 0.4     0.5
代码：svm_prob.py
5.网格搜索
ms.GridSearchCV(模型, 超参数组合列表, cv=折叠数)
    ->模型对象
模型对象.fit(输入集，输出集)
针对超参数组合列表中的每一个超参数组合，实例化给定的模型，做cv次交叉验证，将其中平均f1得分最高的超参数组合作为最佳选择，实例化模型对象。
代码：svm_gs.py
6.事件预测
代码：svm_evt.py
7.交通流量预测(回归)
代码：svm_trf.py
十三、聚类
分类 vs. 聚类
class     cluster
有监督   无监督
1.样本相似性：欧氏距离
欧几里得
《几何原理》
P(x1) - Q(x2): |x1-x2| = sqrt((x1-x2)^2)
P(x1,y1) - Q(x2,y2): sqrt((x1-x2)^2+(y1-y2)^2)
P(x1,y1,z1) - Q(x2,y2,z2):
sqrt((x1-x2)^2+(y1-y2)^2+(z1-z2)^2)
用两个样本对应特征值之差的平方和之平方根，即欧氏距离，来表示这两个样本的相似性。
2.K均值算法
第一步：随机选择k个样本作为k个聚类的中心，计算每个样本到各个聚类中心的欧氏距离，将该样本分配到与之距离最近的聚类中心所在的类别中。
第二步：根据第一步所得到的聚类划分，分别计算每个聚类的几何中心，将几何中心作为新的聚类中心，重复第一步，直到计算所得几何中心与聚类中心重合或接近重合为止。
1)聚类数k必须事先已知。
借助某些评估指标，优选最好的聚类数。
2)聚类中心的初始选择会影响到最终聚类划分的结果。
初始中心尽量选择距离较远的样本。
代码：km.py
图像量化
代码：quant.py
3.均值漂移算法
首先假定样本空间中的每个聚类均服从某种已知的概率分布规则，然后用不同的概率密度函数拟合样本中的统计直方图，不断移动密度函数的中心(均值)的位置，直到获得最佳拟合效果为止。这些概率密度函数的峰值点就是聚类的中心，再根据每个样本距离各个中心的距离，选择最近聚类中心所属的类别作为该样本的类别。
1)聚类数不必事先已知，算法会自动识别出统计直方图的中心数量。
2)聚类中心不依据于最初假定，聚类划分的结果相对稳定。
3)样本空间应该服从某种概率分布规则，否则算法的准确性会大打折扣。
代码：shift.py
4.凝聚层次算法
首先假定每个样本都是一个独立的聚类，如果统计出来的聚类数大于期望的聚类数，则从每个样本出发寻找离自己最近的另一个样本，与之聚集，形成更大的聚类，同时令总聚类数减少，不断重复以上过程，直到统计出来的聚类数达到期望值为止。
1)聚类数k必须事先已知。
借助某些评估指标，优选最好的聚类数。
2)没有聚类中心的概念，因此只能在训练集中划分聚类，但不能对训练集以外的未知样本确定其聚类归属。
3)在确定被凝聚的样本时，除了以距离作为条件以外，还可以根据连续性来确定被聚集的样本。
代码：agglo.py、spiral.py
5.轮廓系数
好的聚类：内密外疏，同一个聚类内部的样本要足够密集，不同聚类之间样本要足够疏远。
电视机
皮夹克
电冰箱
羽绒服
好！
A : 电视机，电冰箱
B : 皮夹克，羽绒服
差！
A : 电视机，羽绒服
B : 电冰箱，皮夹克
针对样本空间中的一个特定样本，计算它与所在聚类其它样本的平均距离a，以及该样本与距离最近的另一个聚类中所有样本的平均距离b，该样本的轮廓系数为(b-a)/max(a, b)，将整个样本空间中所有样本的轮廓系数取算数平均值，作为聚类划分的性能指标s。
  -1 <----- 0 -----> 1 
最差    聚类重叠     最好
sm.silhouette_score(输入集, 输出集,
    sample_size=样本数, metric=距离算法)->平均轮廓系数
    距离算法：euclidean，欧几里得距离
代码：score.py
6.DBSCAN(带噪声的基于密度的聚类)算法
朋友的朋友也是朋友
从样本空间中任意选择一个样本，以事先给定的半径做圆，凡被该圆圈中的样本都视为与该样本处于相同的聚类，以这些被圈中的样本为圆心继续做圆，重复以上过程，不断扩大被圈中样本的规模，直到再也没有新的样本加入为止，至此即得到一个聚类。于剩余样本中，重复以上过程，直到耗尽样本空间中的所有样本为止。
1)事先给定的半径会影响最后的聚类效果，可以借助轮廓系数选择较优的方案。
2)根据聚类的形成过程，把样本细分为以下三类：
外周样本：被其它样本聚集到某个聚类中，但无法再引入新样本的样本。
孤立样本：聚类中的样本数低于所设定的下限，则不称其为聚类，反之称其为孤立样本。
核心样本：除了外周样本和孤立样本以外的样本。
代码：dbscan.py
十四、推荐引擎
1.欧氏距离分数
                                 1
欧氏距离分数 = -------------
                          1+欧氏距离
       欧氏距离->0 ->oo
欧氏距离分数->1 ->0
     0 <-> 1
不相似   相似
    a   b   c   ...
a  x   x   x   ...
b  x   x   x   ...
c  x   x   x   ...
...
代码：es.py
2.皮氏距离分数
相关性矩阵
/         1          相关性系数 \
\  相关性系数           1       /
相关性系数 = 协方差/标准差之积
-1 <------- 0 -------> 1
反相关    不相关   正相关
3-5
0-2
代码：ps.py
3.按照相似度从高到低排列每个用户的相似用户
代码：sim.py
4.生成推荐清单
推荐度
皮氏距离分数>0的用户
打分高低
相似度权重
代码：rcm.py
十五、自然语言
 __________________________________________
 |                                                                ^
 v                                                                |
人->说话-->机         器        女         友->说话
        |          文本--->语义->逻辑-->文本     |
        |语音识别|自然语言| 业务|自然语言|语音合成
NLTK - 自然语言工具包
1.分词
import nltk.tokenize as tk
tk.sent_tokenize(文本)->句子列表
tk.word_tokenize(文本)->单词列表   \
分词器 = tk.WordPunctTokenizer()   > 略有不同
分词器.tokenize(文本)->单词列表      /
代码：tkn.py
2.词干
import nltk.stem.porter as pt
import nltk.stem.lancaster as lc
import nltk.stem.snowball as sb
pt.PorterStemmer() -> 波特词干提取器，偏宽松
lc.LancasterStemmer() -> 朗卡斯特词干提取器，偏严格
sb.SnowballStemmer(语言) -> 思诺博词干提取器，偏中庸
XXX词干提取器.stem(单词)->词干
代码：stm.py
3.原型
名词：复数->单数
动词：分词->原型
代码：lmm.py
4.词袋
The brown dog is running. The black dog is in the black room. Running in the room is forbidden.
1 The brown dog is running
2 The black dog is in the black room
3 Running in the room is forbidden
   the brown dog is running black in room forbidden
1   1      1         1    1      1            0     0     0          0
2   1      0         1    1      0            2     1     1          0
3   1      0         0    1      1            0     1     1          1
代码：bow.py
5.词频
   apple
1 5/8
2 10/100
词袋矩阵的归一化。
代码：tf.py
6.文档频率
含有某个单词的样本数/总样本数
7.逆文档频率
总样本数/含有某个单词的样本数
8.词频-逆文档频率(TF-IDF)
词频矩阵中的每一个元素乘以相应单词的逆文档频率，其值越大说明该词对样本语义的贡献越大，根据每个词的贡献力度，构建学习模型。
代码：tfidf.py
9.文本分类(主题识别)
代码：doc.py
1 2 3 4 5 6
2 3 0 0 1 4
0 4 1 1 2 2
10.性别识别
代码：gndr.py
age score phone
25   80      110
28   90      120
...
/ 25   80  110 \
| 28   90   120 |
\ ...                  /
[{'age': 25, 'score': 80, 'phone': 110},
 {'age': 28, 'score': 90, 'phone': 120}]
11.情感分析
代码：sent.py
12.主题抽取
代码：topic.py
十六、语音识别
声音->文本
1.声音的本质是震动，震动的本质是位移关于时间的函数
Signal: s = f(t)
波形文件(.wav)中记录了不同采样时刻的位移
2.通过傅里叶变换，可以将时间域的声音函数分解为一系列不同频率的正弦函数的叠加，通过频率谱线的特殊分布，建立音频内容和文本的对应关系，以此作为模型训练的基础。
代码：audio.py
3.梅尔频率倒谱系数(MFCC)通过与声音内容密切相关的13个特殊频率所对应的能量分布，作为语音的特征。
代码：mfcc.py
4.语音识别
代码：spch.py































